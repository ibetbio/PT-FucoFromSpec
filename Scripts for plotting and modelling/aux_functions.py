# -*- coding: utf-8 -*-
"""
Created on Tue Jul 18 17:36:43 2023

@author: Pedro

Auxiliary functions for:
    
    - data augmentation
    - fitting ML model (CNN, PLS, XGB, RFR or SVM)
    - running PLS models with windows within the spectra
    - VIP
    - dataset split based on labels
    - plotting accuracy scatters
    - dilution factor application by day/assay
    - data transformations
    - linear calibration

"""

import math
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.cross_decomposition import PLSRegression,PLSCanonical
from sklearn.model_selection import train_test_split,GroupKFold,cross_val_predict
from aux_functions_2 import r2,rmse,plotasEEM2,maxk,make_model
from aux_functions_2 import make_model_2D,reconstructEEMdatabase,plotresultsbyday
import matplotlib.pyplot as plt
from matplotlib import rcParams

from sklearn.ensemble import RandomForestRegressor
from sklearn import svm
import xgboost as xgb

from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
# from skopt import BayesSearchCV




SUB = str.maketrans("0123456789", "₀₁₂₃₄₅₆₇₈₉")
SUP = str.maketrans("0123456789", "⁰¹²³⁴⁵⁶⁷⁸⁹")
Trainsub = str.maketrans("t","ₜ")
badchars = ["(",")","/"," "]

waves = list(range(300,800))

def generate_noise(std,shift,numofsamples):
    return np.random.normal(0, std*shift, numofsamples)

def augment_df_byday(df,numofreps,shift,bioparam):

    """
    Like any experimental procedure, standard analytical methods are susceptible to human error. 
    However, in multi-step processes such as HPLC and cytometry, the potential for error propagation is significant. 
    To deal with this, technical replicates are recommended, but when combined with biological replicates they can excessively increase the total labor and expense. 
    In this work, data augmentation was used to simulate technical replication. Data augmentation is a statistical technique popularly used in machine learning,
    and it provides the ability to simulate an expected form of irrelevant variance in a data set, improving its quality.
    
    In this case, the irrelevant variance to mitigate is human experimental error on the standard analytical methods side.
    
    For this purpose, a 5-fold data augmentation was applied, following these steps:
    1)	Separately collect the data on the Day of each assay (e.g, Day 3 of assay F/2, Day 5 of assay F/2, …, Day 20 of assay F/2+N+P)
    2)	For each, generate normal distributions for fucoxanthin and cell count, based on their individual mean and standard deviation
    3)	Generate 4 artificial technical replicates for each assay’s Day, by random sampling of the generated distribution
    4)	Create 4 new observations per assay/Day, using the new values of Fx and CC, while just repeating the spectroscopy values
    The data augmentation process was only applied for the training subset.

    """
    
    
    augmented_data = pd.DataFrame(columns=df.columns,data=df.values,index = df.index)
    
    if numofreps > 0:
    
        for assay in df['Assay_ori'].unique():
            
            df_work = df[df['Assay_ori'] == assay]
        
            std_by_day = pd.DataFrame(df_work.groupby('Day')[bioparam].std()).rename(columns={bioparam: 'std'})
            mean_by_day = pd.DataFrame(df_work.groupby('Day')[bioparam].mean()).rename(columns={bioparam: 'mean'})
            
            stats_df = pd.merge(mean_by_day, std_by_day, on='Day')
       
            # Version 1 - Artificial samples generated by adding noise
            # for day, std in std_by_day.items():
            #     day_group = df[df['Day'] == day]
            #     for _ in range(numofreps):  # numofreps-fold augmentation
            #         noise = generate_noise(std, shift, len(day_group))
            #         artificial_samples = day_group.copy()
            #         artificial_samples[bioparam] += noise
            #         augmented_data = pd.concat([augmented_data, artificial_samples])
        
            # Version 2 - Artificial samples generated from normal distribution
            for day, row in stats_df.iterrows():
                day_group = df_work[df_work['Day'] == day]
                for _ in range(numofreps):  # numofreps-fold augmentation
                    artificial_values = np.random.normal(row['mean'], row['std']*0.9, len(day_group))
                    artificial_samples = day_group.copy()
                    artificial_samples[bioparam] = artificial_values
                    augmented_data = pd.concat([augmented_data, artificial_samples])


    return augmented_data
        

    
def computeMLmodel(MLproblem,X_train_aug,X_train,y_train_aug,transformation,
                   newpositions_optimal,newEEPs_optimal,MLalg,output2predict,
                   hyperparameters2tune,innerCV_foldby,sampledata,tune,visualizeCNNtrain):

    X_train_aug_labeled = sampledata[[innerCV_foldby]].join(X_train_aug).dropna()   
    y_train_aug_labeled = sampledata[[innerCV_foldby]].join(y_train_aug).dropna() 
    
    groups = X_train_aug_labeled[innerCV_foldby]
    group_kfold = GroupKFold(n_splits=len(groups.unique()))
 
    Xrest_train = X_train[newEEPs_optimal].to_numpy()
    Xrest_train_aug = X_train_aug_labeled[newEEPs_optimal].to_numpy()
    
    y_train_aug = y_train_aug_labeled.iloc[:,1:]
    
    groupCV = group_kfold.split(Xrest_train_aug, y_train_aug, groups.to_numpy())    

    if MLalg == 'CNNR':   
        
        hyperparameters_tuned = hyperparameters2tune 
        
        input_dim = Xrest_train_aug.shape[1]
        model_optimal = make_model(input_dim,hyperparameters_tuned,MLproblem)  
             
        history = model_optimal.fit(Xrest_train_aug,y_train_aug,
                          epochs=hyperparameters2tune.get('learning_epochs'),
                          batch_size=hyperparameters2tune.get('learning_batchsize'),
                          callbacks=hyperparameters2tune.get('rdlr'))


    elif MLalg == 'CNNR_2D':
        
        hyperparameters_tuned = hyperparameters2tune
        Xrest_train_aug = reconstructEEMdatabase(Xrest_train_aug,newpositions_optimal)
        Xrest_train = reconstructEEMdatabase(Xrest_train,newpositions_optimal)
        
        model_optimal = make_model_2D((109,109,1),hyperparameters_tuned,MLproblem)    
                 
        history = model_optimal.fit(Xrest_train_aug,y_train_aug,
                          epochs=hyperparameters2tune.get('learning_epochs'),
                          batch_size=hyperparameters2tune.get('learning_batchsize'),
                          callbacks=hyperparameters2tune.get('rdlr'))                          
        
    
    
    elif MLalg== 'PLSR':
        model_optimal = PLSRegression()
        if tune:
            param_grid = hyperparameters2tune
            search = GridSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
                                  verbose=1).fit(Xrest_train_aug, y_train_aug)

            print("The best hyperparameters are ",search.best_params_)
            hyperparameters_tuned = search.best_params_
        else:
            hyperparameters_tuned = hyperparameters2tune 
        
        model_optimal = PLSRegression(n_components=hyperparameters_tuned["n_components"])
                     

    elif MLalg== 'PLSDA':

        model_optimal = PLSRegression()
        
        y_train_aug['Reverse'] = 1-y_train_aug[output2predict].values          
        y_train_aug = np.array(y_train_aug)        
        
        if tune:
            param_grid = hyperparameters2tune
            search = GridSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
                                  verbose=1).fit(Xrest_train_aug, y_train_aug)

            print("The best hyperparameters are ",search.best_params_)
            hyperparameters_tuned = search.best_params_
        else:
            hyperparameters_tuned = hyperparameters2tune 
        
        model_optimal = PLSRegression(n_components=hyperparameters_tuned["n_components"])
     


    elif MLalg == "RFR":
  
        model_optimal = RandomForestRegressor()       
        if tune:
            param_grid = hyperparameters2tune
            
            # search = GridSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                       verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            search = RandomizedSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
                                        n_iter=50,verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            # search = BayesSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                             n_iter=30,verbose=1).fit(Xrest_train_aug, y_train_aug)           
            
            
            print("The best hyperparameters are ",search.best_params_)
            hyperparameters_tuned = search.best_params_
        else:
            hyperparameters_tuned = hyperparameters2tune 
            
        model_optimal = RandomForestRegressor(n_estimators = hyperparameters_tuned["n_estimators"],
                                              max_depth=hyperparameters_tuned["max_depth"],
                                              min_samples_leaf=hyperparameters_tuned["min_samples_leaf"],
                                              min_samples_split=hyperparameters_tuned["min_samples_split"],
                                              max_features=hyperparameters_tuned["max_features"],
                                              bootstrap=hyperparameters_tuned["bootstrap"],
                                              random_state=42)       
        
        
        
        
    elif MLalg== "XGB":
        
        model_optimal = xgb.XGBRegressor()
        if tune:
            param_grid = hyperparameters2tune
            
            # search = GridSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                       verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            search = RandomizedSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
                                        n_iter=50,verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            # search = BayesSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                             n_iter=30,verbose=1).fit(Xrest_train_aug, y_train_aug)                
        
            print("The best hyperparameters are ",search.best_params_)
            hyperparameters_tuned = search.best_params_
        else:
            hyperparameters_tuned = hyperparameters2tune 
        
        model_optimal = xgb.XGBRegressor(learning_rate = hyperparameters_tuned["learning_rate"],
                                       n_estimators  = hyperparameters_tuned["n_estimators"],
                                       gamma   = hyperparameters_tuned["gamma"],
                                       # reg_alpha     = hyperparameters_tuned["reg_alpha"],
                                       # reg_lambda     = hyperparameters_tuned["reg_lambda"],
                                       min_child_weight     = hyperparameters_tuned["min_child_weight"],
                                       subsample     = hyperparameters_tuned["subsample"],
                                       colsample_bytree     = hyperparameters_tuned["colsample_bytree"],
                                       eval_metric='rmsle',
                                       objective='reg:squarederror')
 

        
    elif MLalg== "SVM":
        
        model_optimal = svm.SVR()
        
        if tune:
            param_grid = hyperparameters2tune
            
            # search = GridSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                       verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            search = RandomizedSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
                                        n_iter=50,verbose=1).fit(Xrest_train_aug, y_train_aug)
            
            # search = BayesSearchCV(model_optimal, param_grid, cv=groupCV,n_jobs=-1,
            #                             n_iter=25,verbose=1).fit(Xrest_train_aug, y_train_aug)    
        
            print("The best hyperparameters are ",search.best_params_)
            hyperparameters_tuned = search.best_params_
        else:
            hyperparameters_tuned = hyperparameters2tune 
            
        model_optimal = svm.SVR(C = hyperparameters_tuned["C"],
                                epsilon = hyperparameters_tuned["epsilon"],
                                kernel = hyperparameters_tuned["kernel"],
                                degree = hyperparameters_tuned["degree"],
                                gamma = hyperparameters_tuned["gamma"])
        
    if MLalg != 'CNNR' and MLalg != "CNNR_2D":
        
        if MLproblem == "Classification" and MLalg == "PLSR":
            dummy = np.array([y_train_aug,1-y_train_aug]).T
            model_optimal.fit(Xrest_train_aug, dummy)
        else:
            model_optimal.fit(Xrest_train_aug, y_train_aug)
        
    
    if visualizeCNNtrain:
        epochs = hyperparameters2tune.get('learning_epochs')
        # visualizing model performance
    
        import matplotlib.pyplot as plt
        from matplotlib import rcParams
    
        rcParams['figure.figsize'] = (18, 8)
        rcParams['axes.spines.top'] = False
        rcParams['axes.spines.right'] = False
    
        plt.plot(
            np.arange(1, epochs+1), 
            history.history['loss'], label='Loss'
        )
        plt.plot(
            np.arange(1, epochs+1), 
            history.history['accuracy'], label='Accuracy'
        )
        plt.plot(
            np.arange(1, epochs+1), 
            history.history['precision'], label='Precision',
        )
        plt.plot(
            np.arange(1, epochs+1), 
            history.history['recall'], label='Recall'
        )
        plt.title('Evaluation metrics', size=20)
        plt.xlabel('Epoch', size=14)
        plt.axis([0,epochs+1,0,1])
        plt.legend()
        plt.show()
            
    
    return model_optimal,Xrest_train,Xrest_train_aug,hyperparameters_tuned


from aux_functions_2 import plot_partialspectrum3

def variselectML(MLproblem,variable_selection,X_ori_train,X_train_aug,y_train_aug,X_train,y_train,transformation,
                 varxID,output2predict,EEPs_selection,EEMpositions,
                 MLalg,hyperparameters2tune,innerCV_foldby,
                 showVS,spectrodata2use,EEM4plot,yplotmax,stdy,movingwindowtop,top_win_tres,log_shift,
                 sampledata,translations,EEM4image):
                       
    
    window_number = 0
    r2train_history = []
    rmseT_history = []
    q2train_history = []
    
    
    if transformation == 'LogOutput' or transformation == 'LogBoth':
        y_train = math.e**y_train-log_shift
        
    X_train_aug_labeled = sampledata[[innerCV_foldby]].join(X_train_aug).dropna()   
    y_train_aug_labeled = sampledata[[innerCV_foldby]].join(y_train_aug).dropna() 
    
      
    groups = X_train_aug_labeled[innerCV_foldby]
    unique_groups = groups.unique() 
    group_kfold = GroupKFold(n_splits=len(unique_groups))
       
    for window in EEPs_selection:
      
        window_number += 1
        
        if spectrodata2use == "Fluoro2D":
            newEEPs = translations[translations.index.isin(window)]['ids']
        else:
            newEEPs = [varxID[item] for item in window]
            
        newpositions = [EEMpositions[n] for n in window]
        

        Xrest_train_aug = X_train_aug_labeled[newEEPs].to_numpy()
        Xrest_train = X_train[newEEPs].to_numpy()
        
        y_train_aug = y_train_aug_labeled.iloc[:,1:]
        
        
        # Inner cross validation for tuning number of LVs
        
        model = PLSRegression()
        
        if MLproblem == 'Classification':
            
            y_train_aug['Reverse'] = 1-y_train_aug[output2predict].values          
            y_train_aug = np.array(y_train_aug)
            
        
        # Xrest_train_aug = sampledata[[innerCV_foldby]].join(X_train_aug).dropna()   
        # y_train_aug = sampledata[[innerCV_foldby]].join(y_train_aug).dropna() 
        
        param_grid = hyperparameters2tune
        search = GridSearchCV(model, param_grid, cv=group_kfold,verbose=1,n_jobs=-1,
                              return_train_score = True).fit(Xrest_train_aug, y_train_aug,groups=groups)
         
        print("The best hyperparameters for partial spectrum PLSR are ",search.best_params_)
        hyperparameters_tuned = search.best_params_
        best_estimator = search.best_estimator_
        
        # yCV_true = []
        # yCV_pred = []
        
        # for train_index, test_index in groupCV:
        #     X_train, X_test = Xrest_train_aug[train_index], Xrest_train_aug[test_index]
        #     y_train, y_test = y_train_aug[train_index], y_train_aug[test_index]
            
        #     best_estimator.fit(X_train, y_train)
        #     yCV_pred.extend(best_estimator.predict(X_test))
        #     yCV_true.extend(y_test)
        
        # yCV_true = np.array(yCV_true)
        # yCV_pred = np.array(yCV_pred)        
        
        yCV_pred = cross_val_predict(best_estimator, Xrest_train_aug, y_train_aug, cv=group_kfold, groups=groups)
                                                                                  
        model = PLSRegression(n_components=hyperparameters_tuned['n_components'])\
                    .fit(X = np.array(Xrest_train_aug), Y = y_train_aug)
                    
        y_predtrain = model.predict(np.array(Xrest_train))
        


 
        if transformation == 'LogOutput' or transformation == 'LogBoth':
            y_predtrain = math.e**y_predtrain -log_shift
            
        if MLproblem == 'Classification':    
            y_predtrain = y_predtrain[:,0]              
            y_predtrain = np.array([1 if x > 0.5 else 0 for x in np.ravel(y_predtrain)])
            winR2 = accuracy_score(y_train,y_predtrain)
            winRMSET = recall_score(y_train,y_predtrain)           
        else:
            winR2 = r2(y_train.values,y_predtrain)
            winRMSET = rmse(y_train.values,y_predtrain)
            winRMSECV = rmse(y_train_aug.values,yCV_pred)
            winQ2 = r2(y_train_aug.values,yCV_pred)        
        
        # winQ2 = search.best_score_
        
        r2train_history.append(winR2)
        q2train_history.append(winQ2)
        rmseT_history.append(winRMSET)    

        if showVS == True:
            
            rcParams['figure.figsize'] = (18, 8)  
            fig,(ax1,ax2) = plt.subplots(1,2)
        
            if spectrodata2use == "Fluoro2D": 
                # plotasEEM2(ax1,newpositions,EEM4plot[newEEPs],'turbo')
                plot_partialspectrum3(ax1,X_ori_train,X_ori_train[newEEPs], EEM4image,vmin=0,vmax=1000)
                
            elif spectrodata2use == 'AbsScan':
                ax1.plot(waves,EEM4plot)
                ax1.scatter(newpositions,EEM4plot[newEEPs],c='C1',s=24,marker='o')
        
          
        
            ax1.set_title(variable_selection + ": iteration " + \
                      str(window_number-1) + " (" + str(len(newEEPs)) + \
                          " variables)")
            # ax1.set_ylim([min(EEM4plot)*0.95,max(EEM4plot)*1.05])
            
            ax2.scatter(y_train_aug.values,yCV_pred,marker='^',c='C1')
            
            ax2.set_ylabel(output2predict+' - Model')
            ax2.set_xlabel(output2predict+' - Experimental')    
            ax2.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
            ax2.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
            ax2.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)

            ax2.set_xlim([0,yplotmax])
            ax2.set_ylim([0,yplotmax])

            ax2.set_title('Train accuracy - '+ MLalg+\
               " with "+variable_selection+"-wise selection")

       
            ax2.text(0.05*yplotmax,0.75*yplotmax,
             "Q2".translate(SUP).translate(Trainsub)+" = "+str(np.round(winQ2,2))+
             "\nRMSET = "+str(np.round(winRMSECV,2)))

            fig.tight_layout(pad=3.5)
            plt.show()


          

           
    if movingwindowtop == 1 or variable_selection == 'VIP':
        top_win = maxk(q2train_history,1)
        window_optima = EEPs_selection[top_win[1]]
    elif variable_selection == 'MW' and movingwindowtop != 1:  
        top_win = maxk(q2train_history,movingwindowtop)
        # top_win = ([x for x in r2train_history if x > top_win_tres],
        #            [i for i, x in enumerate(r2train_history) if x > top_win_tres])
        window_optima = []    
        for win in top_win[1]:
            window_optima += EEPs_selection[win]

    window_optima = list(dict.fromkeys(window_optima))
    print('Window optima is number ',top_win[1])
    
    if spectrodata2use == "Fluoro2D":
        newEEPs_optimal = translations[translations.index.isin(window_optima)]['ids']
    else:
        newEEPs_optimal = [varxID[item] for item in window_optima]
            
    newpositions_optimal = [EEMpositions[n] for n in window_optima]
  

    
    return window_optima,newEEPs_optimal,newpositions_optimal


def vip(x, y, model):
    t = model.x_scores_
    w = model.x_weights_
    q = model.y_loadings_

    m, p = x.shape
    _, h = t.shape

    vips = np.zeros((p,))

    s = np.diag(t.T @ t @ q.T @ q).reshape(h, -1)
    total_s = np.sum(s)

    for i in range(p):
        weight = np.array([ (w[i,j] / np.linalg.norm(w[:,j]))**2 for j in range(h) ])
        vips[i] = np.sqrt(p*(s.T @ weight)/total_s)

    return vips

def obtainVIP_PLSRmodel(MLproblem,X_train_aug,y_train_aug,transformation,hyperparameters2tune,
                        innerCV_foldby,sampledata):
                                                                        
   # Inner cross validation for tuning number of LVs
   
    X_train_aug_labeled = sampledata[[innerCV_foldby]].join(X_train_aug).dropna() 
    y_train_aug_labeled = sampledata[[innerCV_foldby]].join(y_train_aug).dropna() 
    
    groups = X_train_aug_labeled[innerCV_foldby]
    group_kfold = GroupKFold(n_splits=len(groups.unique()))
 
    
    groupCV = group_kfold.split(X_train_aug, y_train_aug, groups.to_numpy())
    model = PLSRegression()
    
    X_train_aug = X_train_aug_labeled.iloc[:,1:]
    y_train_aug = y_train_aug_labeled.iloc[:,1:]
    
    
    param_grid = hyperparameters2tune
    search = GridSearchCV(model, param_grid, cv=groupCV,verbose=1,n_jobs=-1,
                          return_train_score = True).fit(X_train_aug, y_train_aug)
    
    print("The best hyperparameters for full spectrum PLSR are ",search.best_params_)
    hyperparameters_tuned = search.best_params_
    
    # Compute PLSR model with tuned number of LVs and get VIPs
    
    model_novariselect = PLSRegression(n_components=hyperparameters_tuned['n_components'])
    model_novariselect.fit(X = np.array(X_train_aug),Y = y_train_aug)
        
    vips = vip(X_train_aug,y_train_aug,model_novariselect)
    
    return vips




def datasetget_group(df_aux,varxID,output2predict,group_set,criterion):

    df_group = df_aux[df_aux[criterion].isin(group_set)]
    obs_ID_group = df_group.index 
    batch_ID_group = df_group.Batch_ID
    
    Xy_ori_group = df_group[['Day','Assay_ori',output2predict]+varxID]
    y_ori_group = Xy_ori_group[['Day','Assay_ori',output2predict]]
    X_ori_group = Xy_ori_group[['Day','Assay_ori']+varxID]
    
    obs_ID_group = y_ori_group.index
    
    df_group = df_group.loc[y_ori_group.index]
    X_ori_group = X_ori_group.loc[y_ori_group.index]
    
   
    return X_ori_group,y_ori_group,obs_ID_group,batch_ID_group,df_group
            

def datatransform(X_ori,y_ori,transformation,log_shift):
    
    mean = X_ori.mean()
    standard_dev = X_ori.std()
    standard_dev[standard_dev<=1] = 1
    X_processed = X_ori.copy().astype(float)
    y_processed = y_ori.copy().astype(float)
    
    if transformation == 'Ori':       
        print("No transformation applied")
        
    elif transformation == 'MCSN':
        X_processed = (X_processed - mean)*1/standard_dev
                
    elif transformation == 'LogBoth':  
        X_processed[X_processed<0.001] = 0.001
        X_processed = np.log(X_processed)
        
        y_processed[y_processed<=0.001] = 0.001
        y_processed += log_shift
        y_processed = np.log(y_processed.astype(float))
                            
    elif transformation == 'LogOutput':        
        # X_processed = (X_ori - mean)*1/standard_dev

        y_processed[y_processed<0.001] = 0.001
        y_processed += log_shift
        y_processed = np.log(y_processed.astype(float))
            
    return X_processed,y_processed,mean,standard_dev


def datatransform_testset(X_test,y_test,transformation,mean,standard_dev,log_shift):
    
    X_processed = X_test.copy()
    y_processed = y_test.copy()
    
    if transformation == 'Ori':       
        print("No transformation applied")
        
    elif transformation == 'MCSN':
        X_processed = (X_processed - mean)*1/standard_dev
                
    elif transformation == 'LogBoth':  
        X_processed[X_processed<0.001] = 0.001
        X_processed = np.log(X_processed)
        
        y_processed[y_processed<0.001] = 0.001
        y_processed += log_shift
        y_processed = np.log(y_processed.astype(float))
                            
    elif transformation == 'LogOutput':        
        # X_processed = (X_processed - mean)*1/standard_dev
        y_processed[y_processed<0.001] = 0.001
        y_processed += log_shift
        y_processed = np.log(y_processed.astype(float))
        
    return X_processed,y_processed


def plotAccScatters(y_train,y_predtrain,y_test,y_pred,output2predict,
                yplotmax,stdy,valcolor,rsq_train,rsq,rmseT,rmseP,sampledata):
    
    rcParams['figure.figsize'] = (3, 2.5)
    

    plt.scatter(y_train,y_predtrain,c='grey',marker='^')
    plt.scatter(y_test,y_pred,c=valcolor,marker='x') 
    plt.ylabel(output2predict+' - Model')
    plt.xlabel(output2predict+' - Experimental')
    
    plt.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
    plt.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
    plt.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
    
    # plt.legend(['Training','Validation'])
    # plt.xlim([0,yplotmax])
    # plt.ylim([0,yplotmax])
    fig7 = plt.gcf()
    plt.show()
    
    fig8,ax = plt.subplots(figsize=(2, 1))
    ax.set_axis_off()  
    plt.text(0,0.5,
            "R2t".translate(SUP).translate(Trainsub)+" = "+str(np.round(rsq_train,2))+
            "\nRMSET = "+str(np.round(rmseT,2))+'\n'+"R\u00b2 = "+str(np.round(rsq,2))+
            "\nRMSEP = "+str(np.round(rmseP,2)))
    plt.show()
    
    # Seaborn
    
    y = y_test.join(y_pred)
    
    y = sampledata.join(y).dropna().sort_values(by='Assay_ori')
    
    # y = y[y['Assay_ori'].isin(['F2'])]
    
    # Define markers and colors
    markers = ['o','s','^','d']
    colors = ['C0','C1','C2','C4']
    colors = [(1.0, 0.8352941176470589, 0.4745098039215686),
              (0.7490196078431373, 0.5647058823529412, 0),
              (0.4980392156862745, 0.3764705882352941, 0),
              (0.3980392156862745, 0.2764705882352941, 0)]
    
    
    assay = "Assay_ori"
    
    unique_mlalg = sampledata[assay].sort_values().unique()
    

    # unique_mlalg = [str(year) for year in unique_mlalg]
    
    # Create a dictionary for markers and colors
    marker_map = {alg: marker for alg, marker in zip(unique_mlalg, markers)}
    color_map = {alg: color for alg, color in zip(unique_mlalg, colors)}
    
    for alg in unique_mlalg:
        marker = marker_map[alg]
        color = color_map[alg]
        subset = y[y[assay] == alg]
    
        sns.scatterplot(data=subset, x=output2predict, y=output2predict + ' - Model',
                        marker=marker, color=color,label=str(alg),legend=False)
    
    stdy = np.std(y_test)
    # yplotmax = np.max(y_test)
    
    plt.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
    plt.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
    plt.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
    
    plt.xlim([0,yplotmax])
    plt.ylim([0,yplotmax]) 
    plt.ylabel(output2predict+' - Model')
    plt.xlabel(output2predict+' - Experimental')
    
    fig9 = plt.gcf()
    
    
    
    plt.show()
    
    return fig7,fig8,fig9

def plotAccScatters_timewise(sampledata,y_train,y_predtrain,obs_ID_train,y_pred,y_test,
                     obs_ID_test,output2predict,days,valcolor,yplotmax):
    
    rcParams['figure.figsize'] = (4, 3)
    
    aux_train = sampledata.join(pd.DataFrame(y_predtrain,
                                             index=obs_ID_train,
                                             columns=[output2predict+' - predicted'])).dropna()
    aux_train = aux_train.join(pd.DataFrame(y_train,index=obs_ID_train,columns = [output2predict]))
    
    train_model_mean,train_model_std,train_model_max,train_model_min \
        = plotresultsbyday(aux_train,output2predict+' - predicted', days)   
    train_exp_mean,train_exp_std,train_exp_max,train_exp_min \
        = plotresultsbyday(aux_train,output2predict, days)   
        
    aux_test = sampledata.join(pd.DataFrame(y_pred,
                                            index=obs_ID_test,
                                            columns=[output2predict+' - predicted'])).dropna()
    
    aux_test = aux_test.join(pd.DataFrame(y_test,index=obs_ID_test))
    
    test_model_mean,test_model_std,test_model_max,test_model_min \
        = plotresultsbyday(aux_test,output2predict+' - predicted', days)   
    test_exp_mean,test_exp_std,test_exp_max,test_exp_min \
        = plotresultsbyday(aux_test,output2predict, days)        
    
    
    
    # plt.errorbar(days,train_exp_mean,yerr=train_exp_std,
    #               fmt='o',capsize=3,c='grey')
    # plt.errorbar(days,train_model_mean,yerr=train_model_std,
    #               fmt='o',capsize=3,c='C0')
    
    plt.ylabel('average ' + output2predict)
    plt.xlabel('DAI')
    
    plt.xticks([3,5,7,9,12,14,16,18,20])
    
    
    plt.errorbar(days,test_exp_mean,yerr=test_exp_std,
                 fmt='o',capsize=3,c='grey')
    
    plt.errorbar(days,test_model_mean,yerr=test_model_std,
                 fmt='x',capsize=3,c=valcolor)

    plt.ylim([0,yplotmax])
    # plt.xlim([0,12])
    
    # seaborn
    
    

    
    
    fig6 = plt.gcf()    
    plt.show()
    
    return fig6

def plotAccScatter_LOBO(ypred_LOBO,valcolor,output2predict,yplotmax,stdy,q2,rmseCV,Q2samplelabels):
    
    rcParams['figure.figsize'] = (2, 1.85)
    
#     plt.scatter(ypred_LOBO['Experimental'],ypred_LOBO['Model'],c=valcolor,marker='x')   
#     plt.ylabel(output2predict+' - Model')
#     plt.xlabel(output2predict+' - Experimental')
    
#     plt.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
#     plt.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
#     plt.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
#     # plt.title("LOBOCV of "+MLalgorithms_tandem[2]+ " with "+variable_selection\
#                  # + "-wise selection of "+str(len(newEEPs_optimal))+" spectra variables")
#     plt.xlim([0,yplotmax])
#     plt.ylim([0,yplotmax]) 
    
#     fig9 = plt.gcf()
#     plt.show()
    
#     fig10,ax = plt.subplots(figsize=(2, 1))
#     ax.set_axis_off()
#     plt.text(0,0.5,
#     "Q2".translate(SUP)+" = "+str(np.round(q2,2))+\
#     "\nRMSECV = "+str(np.round(rmseCV,2))) #+\
# #        "\n"+spectrodata2use+"\n"+transformation+'_'+MLalg+'_'+varsel)
#     fig10 = plt.gcf()
#     plt.show()
    
    # Seaborn
    
    colors = [(1.0, 0.8352941176470589, 0.4745098039215686),
              (0.7490196078431373, 0.5647058823529412, 0),
              (0.4980392156862745, 0.3764705882352941, 0),
              (0.3980392156862745, 0.2764705882352941, 0)]
    
    # rcParams['figure.figsize'] = (4, 3.5)
    
    sns.scatterplot(data=ypred_LOBO, x = 'Experimental',
                    y = 'Model', hue='Assay_ori',  palette = colors, legend= False)
    
    plt.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
    plt.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
    plt.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
    
    plt.xlim([0,yplotmax])
    plt.ylim([0,yplotmax]) 
    
    # plt.show()
    
    # Plot Q2 with labels
    
    if Q2samplelabels:
    
        fig, ax = plt.subplots()
        ax.scatter(ypred_LOBO['Experimental'],ypred_LOBO['Model'],c=valcolor)
        
        for i,txt in enumerate(list(ypred_LOBO.index)):
            ax.annotate(txt,(ypred_LOBO['Experimental'][i],ypred_LOBO['Model'][i]))
        
        
        ax.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
        ax.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
        ax.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
        
        plt.xlim([0,yplotmax])
        plt.ylim([0,yplotmax]) 
        plt.text(0.05*yplotmax,0.75*yplotmax,
        "Q2".translate(SUP)+" = "+str(np.round(q2,2))+"\nRMSECV = "+str(np.round(rmseCV,2)))
        
        plt.ylabel(output2predict+' - Model')
        plt.xlabel(output2predict+' - Experimental')
        # plt.title("LOBOCV of "+MLalgorithms_tandem[2]+ " with "+variable_selection\
        #               + "-wise selection of "+str(len(newEEPs_optimal))+" spectra variables")
        # fig11 = plt.gcf()
        plt.show()
    
    # return fig9,fig10

import os

def create_folder_if_not_exists(folder_path):
    if not os.path.exists(folder_path):
        try:
            os.makedirs(folder_path)
            print(f"Folder '{folder_path}' created successfully.")
        except OSError as e:
            print(f"Error creating folder: {e}")
    else:
        print(f"Folder '{folder_path}' already exists.")
        
def lighten_color(color, amount=0.5):
    """
    Lightens the given color by multiplying (1-luminosity) by the given amount.
    Input can be matplotlib color string, hex string, or RGB tuple.

    Examples:
    - lighten_color('g', 0.3)
    - lighten_color('#F034A3', 0.6)
    - lighten_color((.3,.55,.1), 0.5)
    """
    import matplotlib.colors as mc
    import colorsys
    try:
        c = mc.cnames[color]
    except:
        c = color
    c = colorsys.rgb_to_hls(*mc.to_rgb(c))
    return colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])

from itertools import combinations
from aux_functions_2 import r2
from scipy.stats import spearmanr,pearsonr

def compute_pairwise_r2(data, groups):
    # Ensure inputs are pandas Series for consistency
    if not isinstance(data, pd.Series):
        data = pd.Series(data)
    if not isinstance(groups, pd.Series):
        groups = pd.Series(groups)
    
    # Get unique group labels
    unique_groups = list(groups.unique())
    unique_groups.sort()
    
    # Prepare a dictionary to hold the results
    pairwise_r2_results = {
        'group1': [],
        'group2': [],
        'r2': [],
        'rmse': [],
        'Pearson R': []
    }
    
    iteration = 1
    # Iterate over all unique pairs of groups
    for group1, group2 in combinations(unique_groups, 2):   
        print(group1, group2)
        
        # Extract data for group1 and group2
        data1 = data[groups == group1].to_frame()
        data1 = pd.DataFrame(data1.values,
                             columns = [group1],
                             index = data1.index)
        data2 = data[groups == group2]
        data2 = pd.DataFrame(data = data2.values,
                             columns = [group2],
                             index = data2.index)
        
        data_global = data1.join(data2)
        
        # Compute accuracy coefficient of determination R²
        rsq = r2(data_global[group1].to_numpy().reshape(-1,1), data_global[group2].to_numpy().reshape(-1,1))
        
        # Compute RMSEP
        
        rmseP = rmse(data_global[group1].to_numpy().reshape(-1,1), data_global[group2].to_numpy().reshape(-1,1))
        
        # Fit linear regression model for each pair and calculate R
        pearsonR,pearsonP = pearsonr(data_global[group1].to_numpy().flatten(),
                                     data_global[group2].to_numpy().flatten())
        
        # figsize = (3.75,3.5)
        # plt.figure(figsize=figsize)

        # plt.scatter(data_global[group1],data_global[group2])
        
        # yplotmax = np.max(data1)*1.25
        # yplotmin = np.min(data1)*0.75
        # stdy = np.std(data1)
        
        # rcParams['figure.figsize'] = (3, 2.5)
  
        # plt.plot([0,yplotmax],[0,yplotmax],'--',color='k',linewidth=0.75) # Y = PredY line
        # plt.plot([0,yplotmax],[stdy,yplotmax+stdy],'--',color='k',linewidth=0.75)
        # plt.plot([0,yplotmax],[-stdy,yplotmax-stdy],'--',color='k',linewidth=0.75)
        
        # plt.xlabel(group1)
        # plt.ylabel(group2)
        # plt.xlim([yplotmin,yplotmax])
        # plt.ylim([yplotmin,yplotmax]) 
        # plt.show()
  
        # Store the results
        pairwise_r2_results['group1'].append(group1)
        pairwise_r2_results['group2'].append(group2)
        pairwise_r2_results['r2'].append(rsq)
        pairwise_r2_results['rmse'].append(rmseP)
        pairwise_r2_results['Pearson R'].append(pearsonR)
        iteration +=1
        

    
    # Convert the results dictionary to a DataFrame and return
    return pd.DataFrame(pairwise_r2_results)


def apply_dilution_factor(df,absorbance_columns, days, assays, dilution_factor,assaytype):
    """
    Applies a dilution factor to specified days and assays in a DataFrame.

    Parameters:
    - df: pandas DataFrame containing the data.
    - days: List of days to apply the dilution factor to.
    - assays: List of assays to apply the dilution factor to.
    - dilution_factor: The dilution factor to apply.

    Returns:
    - A pandas DataFrame with the dilution factor applied to the specified days and assays.
    """
    # Filter the DataFrame for the specified days and assays
    filtered_df = df[(df['Day'].isin(days)) & (df[assaytype].isin(assays))]
    
    # Columns that contain absorbance values
    # absorbance_columns = [str(i) for i in range(300, 800)]  # Assuming columns are named as strings
    
    # Apply the dilution factor to the absorbance values
    df.loc[filtered_df.index, absorbance_columns] = df.loc[filtered_df.index, absorbance_columns].apply(lambda x: x * dilution_factor)
    
    return df


def DWlinearcalibration(monitordata,sampledata,absdata,absextdata):
    

    monitordata["Chl (ppm)"] = absextdata['ext_664']/0.08767
    
    regressiondata = monitordata.join(absdata[['Abs at 750 nm','Abs at 664 nm','Abs at 445 nm']])
    regressiondata['Assay_ID'] = sampledata['Assay_ID']
    regressiondata = regressiondata[regressiondata['Assay_ID'].isin(['PTT2','PTT1'])]

    DW_calcbasis = 'MUSE' # 'OD' #

    #['PTop','PTS2','PTT2','PTT1','PTS1']

    if DW_calcbasis == 'MUSE':

        regressiondata['DW - MUSE estimation (g/L)'] \
            = 1e-3*monitordata['CC (M/mL)'] * monitordata['CytoFSC (a.u.)']
            
        # regressiondata['Chl - MUSE estimation (g/L)'] \
        #     = 1e-8*monitordata['CC (M/mL)'] * monitordata['CytoRed (a.u.)'].pow(2)
        
        # regressiondata['Fx - MUSE estimation (g/L)'] \
        #     = 1e-8*monitordata['CC (M/mL)'] * monitordata['CytoRed (a.u.)'].pow(2)
        
        x = np.array(regressiondata['DW - MUSE estimation (g/L)'])
        # x = np.array(regressiondata['Chl - MUSE estimation (g/L)'])
        # x = np.array(regressiondata['Fx - MUSE estimation (g/L)'])
        fit_intercept = True
        
    elif DW_calcbasis == 'OD750':
        x = np.array(regressiondata['Abs at 750 nm'])
        # x = np.exp(np.array(regressiondata['Abs at 664 nm']))
        # x = np.exp(np.array(regressiondata['Abs at 445 nm'])) 
        
        fit_intercept = False


    y = np.array(regressiondata['DW - experimental (g/L)'])
    # y = np.array(regressiondata["Chl (ppm)"])
    # y = np.array(regressiondata["Fx (ppm)"])

    nan_rows = np.logical_or(np.isnan(x), np.isnan(y))
    x = x[~nan_rows]
    y = y[~nan_rows]
    
    from sklearn.linear_model import LinearRegression

    model0 = LinearRegression(fit_intercept=fit_intercept)  
     
    x = x.reshape(-1,1)
    y = y.reshape(-1,1)

    model0.fit(x,y)


    r_sq0 = model0.score(x,y)
    print('DW Calibration performance: R2 = ',r_sq0)

    if fit_intercept:
        b0 = model0.intercept_[0]
    else:
        b0 = 0
    m0 = model0.coef_[0][0]

    plt.scatter(y,x)

    plt.xlabel('DW exp. (g/L)')
    # plt.xlabel('Chl exp. (mg/L)')
    # plt.xlabel('Fx exp. (mg/L)')

    # plt.text(max(y)*0.5,min(x)*1.5,'R2 = '+str(np.round(r_sq0,3))\
    #          +'\nExperimental = '+str(np.round(m0,3))+' x MUSEestimate + '+str(np.round(b0,3)))
        
    plt.text(max(y)*0.5,min(x)*1.5,'R2 = '+str(np.round(r_sq0,3))\
             +'\nExperimental = '+str(np.round(m0,3))+' x MUSEestimate + '+str(np.round(b0,3)))


    plt.plot([m0*min(x)+b0,m0*max(x)+b0],[min(x),max(x)],color='k',linewidth=0.75,linestyle='--')


    if DW_calcbasis == 'MUSE':
        
        monitordata['DW - MUSE estimation (g/L)'] \
            = 1e-3*monitordata['CC (M/mL)'] * np.log(monitordata['CytoFSC (a.u.)']).pow(3) 
        
        monitordata['DW (g/L)'] = monitordata['DW - MUSE estimation (g/L)']*m0+b0
        # monitordata['Chl (mg/L)'] = regressiondata['Chl - MUSE estimation (g/L)']*m0+b0
        # monitordata['Fx (mg/L)'] = regressiondata['Fx - MUSE estimation (g/L)']*m0+b0
        
        plt.ylabel('DW - MUSE estimation (a.u.)')
        # plt.ylabel('Chl - MUSE estimation (a.u.)')
        # plt.ylabel('Fx - MUSE estimation (a.u.)')
        
    elif DW_calcbasis == 'OD750':
        
        monitordata['DW (g/L)'] = absdata['Abs at 750 nm']*m0+b0
        plt.ylabel('Abs at 750 nm')
        # monitordata['DW (g/L)'] = regressiondata['Abs at 664 nm']*m0+b0
        # plt.ylabel('Abs at 664 nm')
        # monitordata['DW (g/L)'] = regressiondata['Abs at 445 nm']*m0+b0
        # plt.ylabel('Abs at 445 nm')    
        
    monitordata = monitordata.drop(columns=['DW - experimental (g/L)','pH',
                                            'CytoNR (a.u.)','DW - MUSE estimation (g/L)'])

    # monitordata["Fx (ppm) - old estimation"] = absextdata['ext_445']*2.431 + 0.164928091



    # monitordata["Fx/Chl"] = monitordata["Fx (ppm)"]/monitordata["Chl (ppm)"]
        
    # monitordata["Fx (mg/g)"] = monitordata['Fx (ppm)']/monitordata['DW (g/L)']

    # monitordata["Chl (mg/g)"] = monitordata['Chl (ppm)']/monitordata['DW (g/L)']

    # monitordata["DW (pg/cell)"] = monitordata['DW (g/L)']/monitordata['CC (M/mL)']*1000

    # monitordata["Fx (pg/cell)"] = monitordata['Fx (ppm)']/monitordata['CC (M/mL)']

    # monitordata["Chl (pg/cell)"] = monitordata['Chl (ppm)']/monitordata['CC (M/mL)']

    # monitordata['CytoFSC (a.u.)'] = np.log(monitordata['CytoFSC (a.u.)'])
    
    return monitordata

def linearcalibration(sampledata,standard,wave,fit_intercept,logtransform):
    
    data = sampledata.join(standard)
    data = data.join(wave).dropna()
    
    y = np.array(data.iloc[:,-2])
    x = np.array(data.iloc[:,-1])

    from sklearn.linear_model import LinearRegression

    model0 = LinearRegression(fit_intercept=fit_intercept)  
     
    x = x.reshape(-1,1)
    y = y.reshape(-1,1)
    
    if logtransform:
        y += 1
        y = np.log(y)

    model0.fit(x,y)

    r_sq0 = model0.score(x,y)
    print('Calibration performance: R2 = ',r_sq0)

    if fit_intercept:
        b0 = model0.intercept_[0]
    else:
        b0 = 0
    m0 = model0.coef_[0][0]
    
    if logtransform:
        standard_name = 'Log ' + standard.name
    else:
        standard_name = standard.name
    
    plt.scatter(y,x)
    plt.ylabel(wave.name)
    plt.xlabel(standard_name)
    
        
    # plt.text(max(x)*0.9,min(y)*1.25,'R2 = '+str(np.round(r_sq0,3))\
    #          +'\n'+standard.name+' = '+str(np.round(m0,3))+' x '+ wave.name+ ' + '+str(np.round(b0,3)))
    
    print('R2 = '+str(np.round(r_sq0,3))\
              +'\n'+standard_name+' = '+str(np.round(m0,3))+' x '+ wave.name+ ' + '+str(np.round(b0,3)))


    plt.plot([m0*min(x)+b0,m0*max(x)+b0],[min(x),max(x)],color='k',linewidth=0.75,linestyle='--')

   
    return m0,b0
